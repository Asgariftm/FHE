{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dicom,lmdb,cv2, re, sys\n",
      "import os, fnmatch, shutil, subprocess\n",
      "from IPython.utils import io\n",
      "import numpy as np\n",
      "np.random.seed(1234)\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
      "\n",
      "CAFFE_ROOT = \"/home/ali/Src/caffe_FCN/\"\n",
      "caffe_path = os.path.join(CAFFE_ROOT, \"python\")\n",
      "if caffe_path not in sys.path:\n",
      "    sys.path.insert(0, caffe_path)\n",
      "\n",
      "import caffe\n",
      "\n",
      "print(\"\\nSuccessfully imported packages, hooooray!\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully imported packages, hooooray!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SAX_SERIES = {\n",
      "    # challenge training\n",
      "    \"SC-HF-I-1\": \"0004\",\n",
      "    \"SC-HF-I-2\": \"0106\",\n",
      "    \"SC-HF-I-4\": \"0116\",\n",
      "    \"SC-HF-I-40\": \"0134\",\n",
      "    \"SC-HF-NI-3\": \"0379\",\n",
      "    \"SC-HF-NI-4\": \"0501\",\n",
      "    \"SC-HF-NI-34\": \"0446\",\n",
      "    \"SC-HF-NI-36\": \"0474\",\n",
      "    \"SC-HYP-1\": \"0550\",\n",
      "    \"SC-HYP-3\": \"0650\",\n",
      "    \"SC-HYP-38\": \"0734\",\n",
      "    \"SC-HYP-40\": \"0755\",\n",
      "    \"SC-N-2\": \"0898\",\n",
      "    \"SC-N-3\": \"0915\",\n",
      "    \"SC-N-40\": \"0944\",\n",
      "}\n",
      "\n",
      "SUNNYBROOK_ROOT_PATH = \"./Sunnybrook_data/\"\n",
      "\n",
      "TRAIN_CONTOUR_PATH = os.path.join(SUNNYBROOK_ROOT_PATH,\n",
      "                            \"Sunnybrook Cardiac MR Database ContoursPart3\",\n",
      "                            \"TrainingDataContours\")\n",
      "TRAIN_IMG_PATH = os.path.join(SUNNYBROOK_ROOT_PATH,\n",
      "                        \"challenge_training\")\n",
      "\n",
      "def shrink_case(case):\n",
      "    toks = case.split(\"-\")\n",
      "    def shrink_if_number(x):\n",
      "        try:\n",
      "            cvt = int(x)\n",
      "            return str(cvt)\n",
      "        except ValueError:\n",
      "            return x\n",
      "    return \"-\".join([shrink_if_number(t) for t in toks])\n",
      "\n",
      "class Contour(object):\n",
      "    def __init__(self, ctr_path):\n",
      "        self.ctr_path = ctr_path\n",
      "        match = re.search(r\"/([^/]*)/contours-manual/IRCCI-expert/IM-0001-(\\d{4})-icontour-manual.txt\", ctr_path)\n",
      "        self.case = shrink_case(match.group(1))\n",
      "        self.img_no = int(match.group(2))\n",
      "    \n",
      "    def __str__(self):\n",
      "        return \"<Contour for case %s, image %d>\" % (self.case, self.img_no)\n",
      "    \n",
      "    __repr__ = __str__\n",
      "\n",
      "def load_contour(contour, img_path):\n",
      "    filename = \"IM-%s-%04d.dcm\" % (SAX_SERIES[contour.case], contour.img_no)\n",
      "    full_path = os.path.join(img_path, contour.case, filename)\n",
      "    f = dicom.read_file(full_path)\n",
      "    img = f.pixel_array.astype(np.int)\n",
      "    ctrs = np.loadtxt(contour.ctr_path, delimiter=\" \").astype(np.int)\n",
      "    label = np.zeros_like(img, dtype=\"uint8\")\n",
      "    cv2.fillPoly(label, [ctrs], 1)\n",
      "    return img, label\n",
      "    \n",
      "def get_all_contours(contour_path):\n",
      "    contours = [os.path.join(dirpath, f)\n",
      "        for dirpath, dirnames, files in os.walk(contour_path)\n",
      "        for f in fnmatch.filter(files, 'IM-0001-*-icontour-manual.txt')]\n",
      "    print(\"Shuffle data\")\n",
      "    np.random.shuffle(contours)\n",
      "    print(\"Number of examples: {:d}\".format(len(contours)))\n",
      "    extracted = map(Contour, contours)\n",
      "    return extracted\n",
      "\n",
      "def export_all_contours(contours, img_path, lmdb_img_name, lmdb_label_name):\n",
      "    for lmdb_name in [lmdb_img_name, lmdb_label_name]:\n",
      "        db_path = os.path.abspath(lmdb_name)\n",
      "        if os.path.exists(db_path):\n",
      "            shutil.rmtree(db_path)\n",
      "    counter_img = 0\n",
      "    counter_label = 0\n",
      "    batchsz = 100\n",
      "    print(\"Processing {:d} images and labels...\".format(len(contours)))\n",
      "    print (int(np.ceil(len(contours) / float(batchsz))))\n",
      "    for i in xrange(int(np.ceil(len(contours) / float(batchsz)))):\n",
      "        batch = contours[(batchsz*i):(batchsz*(i+1))]\n",
      "        if len(batch) == 0:\n",
      "            break\n",
      "        imgs, labels = [], []\n",
      "        for idx,ctr in enumerate(batch):\n",
      "            try:\n",
      "                img, label = load_contour(ctr, img_path)\n",
      "                imgs.append(img)\n",
      "                labels.append(label)\n",
      "                if idx % 20 == 0:\n",
      "                    print ctr\n",
      "                    plt.imshow(img)\n",
      "                    plt.show()\n",
      "                    plt.imshow(label)\n",
      "                    plt.show()\n",
      "            except IOError:\n",
      "                continue\n",
      "        db_imgs = lmdb.open(lmdb_img_name, map_size=1e12)\n",
      "        with db_imgs.begin(write=True) as txn_img:\n",
      "            for img in imgs:\n",
      "                datum = caffe.io.array_to_datum(np.expand_dims(img, axis=0))\n",
      "                txn_img.put(\"{:0>10d}\".format(counter_img), datum.SerializeToString())\n",
      "                counter_img += 1\n",
      "        print(\"Processed {:d} images\".format(counter_img))\n",
      "        db_labels = lmdb.open(lmdb_label_name, map_size=1e12)\n",
      "        with db_labels.begin(write=True) as txn_label:\n",
      "            for lbl in labels:\n",
      "                datum = caffe.io.array_to_datum(np.expand_dims(lbl, axis=0))\n",
      "                txn_label.put(\"{:0>10d}\".format(counter_label), datum.SerializeToString())\n",
      "                counter_label += 1\n",
      "        print(\"Processed {:d} labels\".format(counter_label))\n",
      "    db_imgs.close()\n",
      "    db_labels.close()\n",
      "\n",
      "if __name__== \"__main__\":\n",
      "    SPLIT_RATIO = 0.1\n",
      "    print(\"Mapping ground truth contours to images...\")\n",
      "    ctrs = get_all_contours(TRAIN_CONTOUR_PATH)\n",
      "    val_ctrs = ctrs[0:int(SPLIT_RATIO*len(ctrs))]\n",
      "    train_ctrs = ctrs[int(SPLIT_RATIO*len(ctrs)):]\n",
      "    print(\"Done mapping ground truth contours to images\")\n",
      "    print(\"\\nBuilding LMDB for train...\")\n",
      "    export_all_contours(train_ctrs, TRAIN_IMG_PATH, \"train_images_lmdb\", \"train_labels_lmdb\")\n",
      "    print(\"\\nBuilding LMDB for val...\")\n",
      "    export_all_contours(val_ctrs, TRAIN_IMG_PATH, \"val_images_lmdb\", \"val_labels_lmdb\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from caffe import layers as L\n",
      "from caffe import params as P\n",
      "\n",
      "n = caffe.NetSpec()\n",
      "\n",
      "# helper functions for common structures\n",
      "def conv_relu(bottom, ks, nout, weight_init='gaussian', weight_std=0.01, bias_value=0, mult=1, stride=1, pad=0, group=1):\n",
      "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
      "                         num_output=nout, pad=pad, group=group,\n",
      "                         weight_filler=dict(type=weight_init, mean=0.0, std=weight_std),\n",
      "                         bias_filler=dict(type='constant', value=bias_value),\n",
      "                         param=[dict(lr_mult=mult, decay_mult=mult), dict(lr_mult=2*mult, decay_mult=0*mult)])\n",
      "    return conv, L.ReLU(conv, in_place=True)\n",
      "\n",
      "def max_pool(bottom, ks, stride=1):\n",
      "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
      "\n",
      "def FCN(images_lmdb, labels_lmdb, batch_size, include_acc=False):\n",
      "    # net definition\n",
      "    n.data = L.Data(source=images_lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=1,\n",
      "                    transform_param=dict(crop_size=0, mean_value=[77], mirror=False))\n",
      "    n.label = L.Data(source=labels_lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=1)\n",
      "    n.conv1, n.relu1 = conv_relu(n.data, ks=5, nout=100, stride=2, pad=50, bias_value=0.1)\n",
      "    n.pool1 = max_pool(n.relu1, ks=2, stride=2)\n",
      "    n.conv2, n.relu2 = conv_relu(n.pool1, ks=5, nout=200, stride=2, bias_value=0.1)\n",
      "    n.pool2 = max_pool(n.relu2, ks=2, stride=2)\n",
      "    n.conv3, n.relu3 = conv_relu(n.pool2, ks=3, nout=300, stride=1, bias_value=0.1)\n",
      "    n.conv4, n.relu4 = conv_relu(n.relu3, ks=3, nout=300, stride=1, bias_value=0.1)\n",
      "    n.drop = L.Dropout(n.relu4, dropout_ratio=0.1, in_place=True)\n",
      "    n.score_classes, _= conv_relu(n.drop, ks=1, nout=2, weight_std=0.01, bias_value=0.1)\n",
      "    n.upscore = L.Deconvolution(n.score_classes)\n",
      "    n.score = L.Crop(n.upscore,n.data)\n",
      "    n.loss = L.SoftmaxWithLoss(n.score, n.label, loss_param=dict(normalize=True))\n",
      "    \n",
      "    if include_acc:\n",
      "        n.accuracy = L.Accuracy(n.score, n.label)\n",
      "        return n.to_proto()\n",
      "    else:\n",
      "        return n.to_proto()\n",
      "\n",
      "def make_nets():\n",
      "    header = 'name: \"FCN\"\\nforce_backward: true\\n'\n",
      "    with open('fcn_train.prototxt', 'w') as f:\n",
      "        f.write(header + str(FCN('train_images_lmdb/', 'train_labels_lmdb/', batch_size=1, include_acc=False)))\n",
      "    with open('fcn_test.prototxt', 'w') as f:\n",
      "        f.write(header + str(FCN('val_images_lmdb/', 'val_labels_lmdb/', batch_size=1, include_acc=True)))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    make_nets()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "caffe.set_mode_cpu() # or caffe.set_mode_cpu() for machines without a GPU\n",
      "try:\n",
      "    del solver # it is a good idea to delete the solver object to free up memory before instantiating another one\n",
      "    solver = caffe.SGDSolver('fcn_solver.prototxt')\n",
      "except NameError:\n",
      "    solver = caffe.SGDSolver('fcn_solver.prototxt')\n",
      "\n",
      "print \"ok!\"\n",
      "# each blob has dimensions batch_size x channel_dim x height x width\n",
      "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}